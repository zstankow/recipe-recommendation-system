{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e578cfd-c6af-405b-8bc9-92da37352185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'recipes'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "\n",
    "es = Elasticsearch('http://localhost:9200')\n",
    "\n",
    "index_name = 'recipes'\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name)\n",
    "\n",
    "with open('recipes.json', 'r') as json_file:\n",
    "    recipes = json.load(json_file)\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"ingredients\": {\"type\": \"text\"},\n",
    "            \"steps\": {\"type\": \"text\"},\n",
    "            \"name\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"text\"},\n",
    "            \"tags\": {\"type\": \"text\"},\n",
    "            \"n_ingredients\": {\"type\": \"text\"},\n",
    "            \"n_steps\": {\"type\": \"text\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"text_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"recipes\"\n",
    "\n",
    "es.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d886af1f-f663-43ec-99b7-4aa9180d49c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3642384642.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    database=es,\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from langchain.chains import ElasticsearchDatabaseChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPEN_AI_API')\n",
    "\n",
    "llm = OpenAI(api_key=openai_api_key, model_name='gpt-3.5-turbo')\n",
    "\n",
    "es_chain = ElasticsearchDatabaseChain((\n",
    "    database=es, \n",
    "    top_k = 5\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant who provides recipes based on user queries.\n",
    "    Context: {context}\n",
    "    User Query: {query}\n",
    "    Please provide a detailed response based on the context and query.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "vector_store = ElasticsearchVectorStore(\n",
    "    es_client=es_client,\n",
    "    index_name=index_name,\n",
    "    embedding_dim=384,\n",
    "    embedding_field=\"text_vector\",\n",
    "    content_fields=[\"name\", \"description\", \"steps\", \"ingredients\", \"id\"]\n",
    ")\n",
    "\n",
    "\n",
    "chain = RetrievalQA(\n",
    "    retriever=vector_store,\n",
    "    prompt_template=prompt_template,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "def rag(query):\n",
    "    response = chain(query)\n",
    "    return response['result']\n",
    "\n",
    "# Example usage\n",
    "query = \"I have sausages, apples, and potatoes. What can I make?\"\n",
    "answer = rag(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2243a9ae-adb1-4f22-b1d6-26bfef366f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zzelk\\anaconda3\\envs\\recipes\\lib\\site-packages\\langchain_community\\llms\\openai.py:253: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIChat\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m load_dotenv()\n\u001b[0;32m      9\u001b[0m openai_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPEN_AI_API\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_api_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m query_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[0;32m     13\u001b[0m     llm\u001b[38;5;241m=\u001b[39mOpenAI(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_openai_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     14\u001b[0m     prompt_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant. Answer the following question based on the context: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[38;5;124m Question: \u001b[39m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Define your answer chain\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zzelk\\anaconda3\\envs\\recipes\\lib\\site-packages\\langchain_community\\llms\\openai.py:258\u001b[0m, in \u001b[0;36mBaseOpenAI.__new__\u001b[1;34m(cls, **data)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    251\u001b[0m     model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name:\n\u001b[0;32m    253\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to use a chat model. This way of initializing it is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno longer supported. Instead, please use: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from langchain_community.chat_models import ChatOpenAI`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     )\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenAIChat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zzelk\\anaconda3\\envs\\recipes\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:203\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     emit_warning()\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zzelk\\anaconda3\\envs\\recipes\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for OpenAIChat\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from langchain.chains.elasticsearch_database.base import ElasticsearchDatabaseChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPEN_AI_API')\n",
    "llm = OpenAI(api_key=openai_api_key, model_name='gpt-3.5-turbo')\n",
    "\n",
    "query_chain = LLMChain(\n",
    "    llm=OpenAI(api_key=\"your_openai_api_key\"),\n",
    "    prompt_template=\"You are a helpful assistant. Answer the following question based on the context: {context} Question: {query}\"\n",
    ")\n",
    "\n",
    "# Define your answer chain\n",
    "answer_chain = LLMChain(\n",
    "    llm=OpenAI(api_key=\"your_openai_api_key\"),\n",
    "    prompt_template=\"Generate an answer based on the retrieved documents: {documents} and the question: {query}\"\n",
    ")\n",
    "\n",
    "# Configure ElasticsearchDatabaseChain\n",
    "db_chain = ElasticsearchDatabaseChain(\n",
    "    answer_chain=answer_chain,\n",
    "    query_chain=query_chain,\n",
    "    database=es,\n",
    "    top_k=5,  # Number of top results to retrieve\n",
    "    verbose=False  # Enable verbose mode for logging\n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    \"context\": \"Information about recipes\",\n",
    "    \"query\": \"How do I make a chocolate cake?\"\n",
    "}\n",
    "\n",
    "response = db_chain(inputs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a2de0-3d7c-4619-b559-678814e14489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
